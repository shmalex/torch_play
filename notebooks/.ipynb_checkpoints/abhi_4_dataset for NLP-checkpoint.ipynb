{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This JN is based on the Abhishek Thakur video\n",
    "https://www.youtube.com/watch?v=BLwvrcaD4GE&list=PL98nY_tJQXZln8spB5uTZdKN08mYGkOf2&index=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to design custom data class for simple NLP problems.\n",
    "classification\n",
    "regression\n",
    "multilable classification\n",
    "multiclass classification\n",
    "question and answering\n",
    "summarizatoin\n",
    "seq-to-seq\n",
    "entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T22:17:25.136238Z",
     "start_time": "2021-05-14T22:17:25.116554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification or regression problems \n",
    "class CustomDataset:\n",
    "    def __init__(self, data, targets, tokenizer):\n",
    "        \"\"\" You can use tokenizers from \n",
    "            Transformers, attention mask, token type ids\"\"\"\n",
    "        self.data = data\n",
    "        self.targets = targrets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem(self, idx):\n",
    "        text = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        # bindary 0,1\n",
    "        # multiclass 0,1,2,3,4\n",
    "        # regression .1, 0.8,...\n",
    "        # multilabel [1, 0, 0, 1, 0]\n",
    "        #\n",
    "        # for entity (multiclass problem):\n",
    "        # target = self.targets[idx, :]\n",
    "        \n",
    "        \n",
    "        # we allways convert text to tokens\n",
    "        # the input_ids list of tokens got from tokenizer\n",
    "        # [101,23,545,113, ...]\n",
    "        # different lengths\n",
    "        input_ids = tokenizer(text)\n",
    "        \n",
    "        # since every document has different length, we should care about padding\n",
    "        # assume for now that input_ids are all same size\n",
    "         \n",
    "        \n",
    "        # we have to return tensors\n",
    "        # target dtype depends on problem:\n",
    "        #    classification torch.long\n",
    "        #    regression torch.float\n",
    "        return {\n",
    "            \"text\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            # if we have attention mask tokenizer\n",
    "            # \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"target\": torch.tensor(target)\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
